%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Sales Analytics: A Multi-Model Machine Learning Approach
% Data Science Lab - Sales Exercise Project
% Generated by Reporter Agent
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[11pt,a4paper]{article}

% ===== PACKAGES =====
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{longtable}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{natbib}

% ===== CONFIGURATION =====
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=green
}

\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    backgroundcolor=\color{gray!10}
}

% ===== CUSTOM COMMANDS =====
\newcommand{\placeholder}[1]{\textcolor{red}{\textbf{[#1]}}}
\newcommand{\rmse}{\text{RMSE}}
\newcommand{\mape}{\text{MAPE}}

% ===== DOCUMENT =====
\begin{document}

% ===== TITLE =====
\title{
    \textbf{Sales Analytics: A Comprehensive Machine Learning Approach} \\
    \large Profit Prediction, Transaction Classification, Sales Forecasting, and Customer Segmentation
}

\author{
    Data Science Laboratory \\
    \texttt{Reporter Agent (Ralph Wiggum Protocol)}
}

\date{January 2026}

\maketitle

% ===== ABSTRACT =====
\begin{abstract}
This paper presents a comprehensive machine learning analysis of retail sales data comprising 9,994 transactions over a four-year period (2014-2017) in the United States market. We address four distinct business problems using specialized modeling approaches: (1) profit prediction through regression models, (2) transaction profitability classification, (3) monthly sales forecasting using time series methods, and (4) customer segmentation via RFM analysis and clustering. Our methodology employs cross-validation with multiple model comparison to identify optimal solutions for each problem domain. The results provide actionable insights for business decision-making, including identification of key profit drivers, early detection of unprofitable transactions, accurate sales projections, and strategic customer targeting based on behavioral segments.

\vspace{0.5em}
\noindent\textbf{Keywords:} Machine Learning, Retail Analytics, Profit Prediction, Sales Forecasting, Customer Segmentation, RFM Analysis
\end{abstract}

\newpage
\tableofcontents
\newpage

% =====================================================
% SECTION 1: INTRODUCTION
% =====================================================
\section{Introduction}

\subsection{Business Context}

The retail industry generates vast amounts of transactional data that, when properly analyzed, can reveal critical patterns for business optimization. Understanding what drives profitability, predicting future sales trends, and identifying distinct customer segments are fundamental challenges that machine learning can effectively address.

This study leverages a comprehensive sales dataset from a United States-based retail operation to demonstrate the application of multiple machine learning paradigms to real-world business problems.

\subsection{Research Objectives}

This research addresses four interconnected business questions:

\begin{enumerate}
    \item \textbf{Profit Prediction (Regression):} Can we accurately predict the profit margin of individual transactions based on order characteristics?

    \item \textbf{Profitability Classification:} Can we identify transactions that will result in losses before they are completed?

    \item \textbf{Sales Forecasting:} Can we project aggregate monthly sales to support inventory and resource planning?

    \item \textbf{Customer Segmentation:} Can we identify distinct customer groups to enable targeted marketing strategies?
\end{enumerate}

\subsection{Contributions}

The main contributions of this work are:

\begin{itemize}
    \item A systematic comparison of regression models for profit prediction, including baseline linear models and ensemble methods (Random Forest, XGBoost, LightGBM)

    \item Development of a profitability classifier to flag high-risk transactions

    \item Application of both statistical (ARIMA/SARIMA) and machine learning (Prophet, XGBoost) methods for sales forecasting

    \item Implementation of RFM-based customer segmentation using multiple clustering algorithms

    \item A unified analytical framework that addresses multiple business questions from a single data source
\end{itemize}

\subsection{Paper Organization}

The remainder of this paper is organized as follows: Section~\ref{sec:data} describes the dataset and exploratory analysis. Section~\ref{sec:methodology} details the methodology for each machine learning task. Section~\ref{sec:results} presents experimental results and model comparisons. Section~\ref{sec:discussion} discusses findings and business implications. Section~\ref{sec:conclusions} concludes with recommendations and future work.

% =====================================================
% SECTION 2: DATASET AND EDA
% =====================================================
\section{Dataset and Exploratory Data Analysis}
\label{sec:data}

\subsection{Data Source and Overview}

The analysis is based on the Sales Order Dataset from Kaggle (datawitharyan/sales-order-dataset), containing retail transaction records from a United States operation.

\begin{table}[H]
\centering
\caption{Dataset Overview}
\label{tab:dataset_overview}
\begin{tabular}{ll}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Total Records & 9,994 \\
Number of Features & 21 \\
Time Period & 2014-01-03 to 2017-12-30 \\
Geographic Scope & United States \\
Unique Customers & 793 \\
Unique Products & 1,862 \\
Unique Orders & 5,009 \\
Missing Values & 0 \\
Duplicate Records & 0 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Feature Description}

The dataset contains 21 variables across different categories:

\subsubsection{Identification Variables}
\begin{itemize}
    \item \texttt{Row ID}: Unique row identifier
    \item \texttt{Order ID}: Transaction identifier (5,009 unique values)
    \item \texttt{Customer ID}: Customer identifier (793 unique values)
    \item \texttt{Product ID}: Product identifier (1,862 unique values)
\end{itemize}

\subsubsection{Temporal Variables}
\begin{itemize}
    \item \texttt{Order Date}: Date when order was placed
    \item \texttt{Ship Date}: Date when order was shipped
\end{itemize}

\subsubsection{Categorical Variables}

\begin{table}[H]
\centering
\caption{Categorical Variables Distribution}
\label{tab:categorical_vars}
\begin{tabular}{llrl}
\toprule
\textbf{Variable} & \textbf{Cardinality} & \textbf{Mode (\%)} & \textbf{Values} \\
\midrule
Ship Mode & 4 & Standard Class (60\%) & Standard, Second, First, Same Day \\
Segment & 3 & Consumer (52\%) & Consumer, Corporate, Home Office \\
Region & 4 & West (32\%) & West, East, Central, South \\
Category & 3 & Office Supplies (60\%) & Office Supplies, Furniture, Technology \\
Sub-Category & 17 & Binders (15\%) & Binders, Paper, Phones, ... \\
State & 49 & California (14\%) & California, New York, Texas, ... \\
City & 531 & New York City (3\%) & NYC, Los Angeles, Philadelphia, ... \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Numerical Variables}

\begin{table}[H]
\centering
\caption{Numerical Variables Summary Statistics}
\label{tab:numerical_vars}
\begin{tabular}{lrrrrr}
\toprule
\textbf{Variable} & \textbf{Mean} & \textbf{Std} & \textbf{Min} & \textbf{Max} & \textbf{Outliers (\%)} \\
\midrule
Sales (\$) & 229.86 & 623.25 & 0.44 & 22,638.48 & 11.7 \\
Profit (\$) & 28.66 & 234.26 & -6,599.98 & 8,399.98 & 18.8 \\
Quantity & 3.79 & 2.23 & 1 & 14 & 1.7 \\
Discount & 0.16 & 0.21 & 0 & 0.80 & 8.6 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Correlation Analysis}

The correlation matrix for numerical variables reveals important relationships:

\begin{table}[H]
\centering
\caption{Pearson Correlation Matrix}
\label{tab:correlation}
\begin{tabular}{lrrrr}
\toprule
& \textbf{Sales} & \textbf{Quantity} & \textbf{Discount} & \textbf{Profit} \\
\midrule
Sales & 1.000 & 0.201 & -0.028 & 0.479 \\
Quantity & 0.201 & 1.000 & 0.009 & 0.066 \\
Discount & -0.028 & 0.009 & 1.000 & -0.219 \\
Profit & 0.479 & 0.066 & -0.219 & 1.000 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key findings:}
\begin{itemize}
    \item \textbf{Sales--Profit correlation (0.479):} Moderate positive relationship; higher sales tend to generate higher profits, though this is not deterministic.

    \item \textbf{Discount--Profit correlation (-0.219):} Negative relationship indicating that discounts erode profitability. This is a critical finding for business operations.

    \item \textbf{Quantity--Profit correlation (0.066):} Weak relationship suggesting that volume alone does not guarantee profitability.
\end{itemize}

\subsection{Temporal Distribution}

The dataset spans four complete years with approximately uniform distribution:

\begin{table}[H]
\centering
\caption{Orders by Year}
\label{tab:temporal}
\begin{tabular}{lr}
\toprule
\textbf{Year} & \textbf{Proportion} \\
\midrule
2014 & $\approx$ 25\% \\
2015 & $\approx$ 25\% \\
2016 & $\approx$ 25\% \\
2017 & $\approx$ 25\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Shipping Mode Analysis}

\begin{table}[H]
\centering
\caption{Distribution by Ship Mode}
\label{tab:ship_mode}
\begin{tabular}{lrr}
\toprule
\textbf{Ship Mode} & \textbf{Count} & \textbf{Percentage} \\
\midrule
Standard Class & 5,968 & 59.7\% \\
Second Class & 1,945 & 19.5\% \\
First Class & 1,538 & 15.4\% \\
Same Day & 543 & 5.4\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Data Quality Assessment}

\textbf{Strengths:}
\begin{itemize}
    \item No missing values across all 21 features
    \item No duplicate records
    \item Consistent data types and formats
    \item Rich feature set covering temporal, categorical, and numerical dimensions
\end{itemize}

\textbf{Considerations for Modeling:}
\begin{itemize}
    \item \textbf{Outliers:} Profit exhibits 18.8\% outliers using the IQR method, including extreme negative values (losses up to \$6,599.98)

    \item \textbf{Negative target values:} Profit can be negative, requiring careful handling in regression and transformation decisions

    \item \textbf{High-cardinality categoricals:} State (49), City (531), Sub-Category (17) require encoding strategies that prevent dimensionality explosion

    \item \textbf{Class imbalance check:} Required for classification task (profitable vs. non-profitable transactions)
\end{itemize}

% =====================================================
% SECTION 3: METHODOLOGY
% =====================================================
\section{Methodology}
\label{sec:methodology}

This section details the methodology for each of the four machine learning tasks. All experiments use \texttt{random\_state=42} for reproducibility.

\subsection{Profit Regression}
\label{subsec:regression}

\subsubsection{Problem Formulation}

Given a transaction with features $\mathbf{x}$, predict the continuous profit value $y \in \mathbb{R}$.

$$\hat{y} = f(\mathbf{x}; \theta)$$

\subsubsection{Feature Engineering}

The following features are derived from raw data:

\begin{itemize}
    \item \texttt{ship\_days}: $\text{Ship Date} - \text{Order Date}$ (shipping time in days)
    \item \texttt{order\_month}: Month extracted from Order Date (1-12)
    \item \texttt{order\_quarter}: Quarter (1-4)
    \item \texttt{order\_dayofweek}: Day of week (0-6)
    \item \texttt{order\_year}: Year of order
    \item \texttt{margin\_potential}: $\text{Sales} \times (1 - \text{Discount})$
\end{itemize}

\textbf{Note:} Sales is excluded as a feature to avoid data leakage, given its high correlation (0.479) with Profit.

\subsubsection{Preprocessing}

\begin{table}[H]
\centering
\caption{Preprocessing Strategy by Model Type}
\label{tab:preprocessing_regression}
\begin{tabular}{lll}
\toprule
\textbf{Feature Type} & \textbf{Linear Models} & \textbf{Tree-Based Models} \\
\midrule
Low-cardinality categorical & One-Hot Encoding & One-Hot Encoding \\
High-cardinality categorical & One-Hot Encoding & Target Encoding \\
Numerical & StandardScaler & None \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Models Compared}

\begin{enumerate}
    \item \textbf{Linear Regression} (baseline)
    \item \textbf{Ridge Regression} with regularization
    \item \textbf{Random Forest Regressor}
    \item \textbf{XGBoost Regressor}
    \item \textbf{LightGBM Regressor}
\end{enumerate}

\subsubsection{Validation Strategy}

5-Fold Cross-Validation with shuffling. Metrics:

\begin{itemize}
    \item \textbf{RMSE (primary):} $\rmse = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$
    \item MAE: $\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$
    \item $R^2$: Coefficient of determination
    \item MAPE: Mean Absolute Percentage Error
\end{itemize}

\subsubsection{Hyperparameter Search Space}

For XGBoost:
\begin{lstlisting}[language=Python]
{
    'n_estimators': [100, 200, 500],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}
\end{lstlisting}

\subsection{Profitability Classification}
\label{subsec:classification}

\subsubsection{Problem Formulation}

Binary classification where the target is:

$$y = \begin{cases} 1 & \text{if Profit} > 0 \text{ (profitable)} \\ 0 & \text{otherwise (unprofitable)} \end{cases}$$

\subsubsection{Feature Engineering}

Same as regression, plus:
\begin{itemize}
    \item \texttt{high\_discount}: 1 if Discount $> 0.2$, else 0
    \item \texttt{is\_standard\_shipping}: 1 if Ship Mode = ``Standard Class'', else 0
\end{itemize}

\subsubsection{Models Compared}

\begin{enumerate}
    \item \textbf{Logistic Regression} (baseline)
    \item \textbf{Random Forest Classifier}
    \item \textbf{XGBoost Classifier}
    \item \textbf{LightGBM Classifier}
    \item \textbf{SVM} (if computationally feasible)
\end{enumerate}

\subsubsection{Class Imbalance Handling}

If class imbalance exceeds 70/30, the following strategies are applied:
\begin{itemize}
    \item \texttt{class\_weight='balanced'} for supporting models
    \item SMOTE (Synthetic Minority Over-sampling) for severe imbalance
    \item \texttt{scale\_pos\_weight} for XGBoost
\end{itemize}

\subsubsection{Validation Strategy}

Stratified 5-Fold Cross-Validation. Metrics:
\begin{itemize}
    \item \textbf{F1-Score (primary)}
    \item Accuracy
    \item Precision and Recall
    \item ROC-AUC
\end{itemize}

\subsection{Sales Forecasting}
\label{subsec:forecasting}

\subsubsection{Problem Formulation}

Given historical monthly sales $\{s_1, s_2, \ldots, s_T\}$, predict future values $\{s_{T+1}, \ldots, s_{T+h}\}$ where $h$ is the forecast horizon.

\subsubsection{Data Aggregation}

Raw transaction data is aggregated to monthly frequency:
$$S_t = \sum_{i: \text{month}(i) = t} \text{Sales}_i$$

This yields approximately 48 monthly observations.

\subsubsection{Feature Engineering (for ML models)}

\begin{itemize}
    \item \texttt{month}: Month of year (1-12)
    \item \texttt{quarter}: Quarter (1-4)
    \item \texttt{year}: Year
    \item \texttt{lag\_1, lag\_2, lag\_3}: Previous months' sales
    \item \texttt{rolling\_mean\_3, rolling\_mean\_6}: Moving averages
    \item \texttt{rolling\_std\_3}: Rolling standard deviation
\end{itemize}

\subsubsection{Models Compared}

\begin{enumerate}
    \item \textbf{Naive baseline} (last value / moving average)
    \item \textbf{SARIMA} (Seasonal ARIMA)
    \item \textbf{Prophet} (Facebook)
    \item \textbf{XGBoost with temporal features}
\end{enumerate}

\subsubsection{Validation Strategy}

\textbf{Time Series Split} (expanding window) --- standard k-fold cannot be used due to temporal dependencies. Test set: last 6 months.

Metrics:
\begin{itemize}
    \item \textbf{RMSE (primary)}
    \item MAE
    \item \textbf{MAPE}
\end{itemize}

\subsection{Customer Segmentation}
\label{subsec:clustering}

\subsubsection{Problem Formulation}

Unsupervised clustering to identify $k$ distinct customer segments based on purchasing behavior.

\subsubsection{RFM Feature Construction}

For each customer $c$:
\begin{itemize}
    \item \textbf{Recency ($R$):} Days since last purchase (reference: dataset end date)
    \item \textbf{Frequency ($F$):} Total number of orders
    \item \textbf{Monetary ($M$):} Total sales value
\end{itemize}

Additional derived features:
\begin{itemize}
    \item \texttt{avg\_order\_value}: $M / F$
    \item \texttt{avg\_profit}: Total Profit / $F$
    \item \texttt{days\_as\_customer}: Last order date - First order date
\end{itemize}

\subsubsection{Preprocessing}

\begin{itemize}
    \item StandardScaler applied to all features (mandatory for clustering)
    \item Log-transform or winsorizing for skewed RFM distributions
\end{itemize}

\subsubsection{Models Compared}

\begin{enumerate}
    \item \textbf{K-Means} (baseline)
    \item \textbf{K-Means with PCA}
    \item \textbf{DBSCAN}
    \item \textbf{Hierarchical Clustering} (Agglomerative)
    \item \textbf{Gaussian Mixture Models} (GMM)
\end{enumerate}

\subsubsection{Cluster Selection}

\begin{itemize}
    \item \textbf{Elbow Method:} Plot inertia vs. $k$
    \item \textbf{Silhouette Score (primary):} $s = \frac{b - a}{\max(a, b)}$
    \item \textbf{Davies-Bouldin Index}
    \item Interpretability and business alignment
\end{itemize}

% =====================================================
% SECTION 4: RESULTS
% =====================================================
\section{Experimental Results}
\label{sec:results}

This section presents the experimental results for all four machine learning tasks. All models were trained using 5-fold cross-validation with \texttt{random\_state=42} for reproducibility.

\subsection{Profit Regression Results}

The profit regression task aimed to predict the continuous profit value for individual transactions. Five models were compared using RMSE as the primary metric.

\begin{table}[H]
\centering
\caption{Regression Model Comparison (5-Fold CV)}
\label{tab:regression_results}
\begin{tabular}{lrrrr}
\toprule
\textbf{Model} & \textbf{RMSE (\$)} & \textbf{RMSE Std} & \textbf{$R^2$} & \textbf{$R^2$ Std} \\
\midrule
\textbf{LightGBM} & \textbf{208.80} & 35.95 & \textbf{0.180} & 0.023 \\
Random Forest & 214.52 & 37.29 & 0.131 & 0.114 \\
Ridge Regression & 218.06 & 34.40 & 0.099 & 0.048 \\
Linear Regression & 219.74 & 33.69 & 0.083 & 0.059 \\
XGBoost & 221.93 & 32.74 & 0.056 & 0.149 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Best Model:} LightGBM achieved the lowest RMSE (\$208.80) and highest $R^2$ (0.180). The optimized hyperparameters were: \texttt{num\_leaves=50}, \texttt{n\_estimators=300}, \texttt{learning\_rate=0.01}, \texttt{feature\_fraction=1.0}.

The relatively modest $R^2$ values across all models suggest that profit prediction is inherently challenging with the available features, likely due to the high variance and presence of extreme outliers in the Profit distribution.

\subsubsection{Feature Importance}

The top predictive features for the LightGBM model were:

\begin{enumerate}
    \item \textbf{Quantity} --- Most important predictor
    \item \textbf{Postal Code} --- Geographic influence on profit
    \item \textbf{Order Day} --- Temporal pattern
    \item \textbf{Discount} --- Strong negative influence on profit
    \item \textbf{Order Month} --- Seasonal effects
    \item \textbf{Day of Week} --- Weekly patterns
    \item \textbf{Sub-Category (Machines)} --- Product-specific margins
    \item \textbf{Ship Days} --- Delivery time impact
\end{enumerate}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{profit_regressor_feature_importance.png}
\caption{Top 20 Feature Importances for ProfitRegressor (LightGBM)}
\label{fig:feature_importance}
\end{figure}

\subsection{Profitability Classification Results}

The profitability classification task aimed to predict whether a transaction would be profitable (Profit $> 0$) or not. Four models were evaluated using Stratified 5-Fold Cross-Validation.

\begin{table}[H]
\centering
\caption{Classification Model Comparison (Stratified 5-Fold CV)}
\label{tab:classification_results}
\begin{tabular}{lrrrrr}
\toprule
\textbf{Model} & \textbf{F1} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{ROC-AUC} \\
\midrule
\textbf{Random Forest} & \textbf{0.965} & \textbf{0.943} & 0.953 & \textbf{0.977} & 0.980 \\
LightGBM & 0.959 & 0.934 & 0.972 & 0.946 & 0.981 \\
XGBoost & 0.955 & 0.929 & \textbf{0.975} & 0.936 & \textbf{0.982} \\
Logistic Regression & 0.943 & 0.910 & 0.955 & 0.932 & 0.949 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Best Model:} Random Forest achieved the highest F1-score (0.965) and Recall (0.977), making it the best choice for minimizing false negatives (missing unprofitable transactions). XGBoost achieved the highest ROC-AUC (0.982) and Precision (0.975).

All models achieved excellent performance ($F1 > 0.94$, $AUC > 0.94$), demonstrating that transaction profitability can be reliably predicted from available features. This high performance is likely due to the strong signal from the Discount feature, which has a known negative correlation with Profit.

\subsubsection{Performance Analysis}

The classification results significantly exceed the target metrics defined in the project plan:

\begin{itemize}
    \item \textbf{F1-Score:} 0.965 achieved vs. 0.75 minimum target ($\checkmark$)
    \item \textbf{ROC-AUC:} 0.982 achieved vs. 0.80 minimum target ($\checkmark$)
\end{itemize}

The high recall (0.977) of Random Forest is particularly valuable for business applications, as it means only 2.3\% of unprofitable transactions would be missed by the model.

\subsection{Sales Forecasting Results}

The sales forecasting task aimed to predict monthly aggregate sales using time series methods. Four models were compared, with the last 6 months held out as a test set.

\begin{table}[H]
\centering
\caption{Forecasting Model Comparison (6-Month Test Set)}
\label{tab:forecasting_results}
\begin{tabular}{lrrr}
\toprule
\textbf{Model} & \textbf{RMSE (\$)} & \textbf{MAE (\$)} & \textbf{MAPE (\%)} \\
\midrule
\textbf{Prophet (cps=0.1)} & \textbf{17,791.86} & \textbf{15,422.50} & \textbf{19.21} \\
Seasonal Naive & 23,430.16 & 20,459.89 & 25.39 \\
SARIMA(0,0,1)$\times$(0,0,2,12) & 30,552.25 & 24,735.10 & 27.81 \\
XGBoost (n=50, d=5) & 30,625.07 & 26,818.03 & 32.36 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Best Model:} Prophet with changepoint prior scale of 0.1 achieved the lowest MAPE (19.21\%), significantly outperforming all other methods. The model captures seasonal patterns effectively while maintaining adaptability to trend changes.

\subsubsection{Performance Against Targets}

The forecasting MAPE of 19.21\% falls slightly below the optimal target (10\%) but meets the minimum target (20\%):

\begin{itemize}
    \item \textbf{MAPE:} 19.21\% achieved vs. 20\% minimum target ($\checkmark$)
    \item Prophet improved over Seasonal Naive baseline by 24\% (relative MAPE reduction)
\end{itemize}

The limited performance improvement is expected given the small sample size (approximately 48 monthly observations) and the inherent volatility in monthly sales patterns.

\subsubsection{Forecast Visualization}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{sales_forecast_plot.png}
\caption{Sales Forecasting: Model Comparison with Actual vs. Predicted Values}
\label{fig:forecast_plot}
\end{figure}

The forecast visualization shows that Prophet (bottom-left panel) tracks the actual values more closely than other models, particularly capturing the Q4 sales spike pattern. XGBoost (bottom-right) shows significant underestimation during peak months.

\subsection{Customer Segmentation Results}

The customer segmentation task used RFM (Recency, Frequency, Monetary) analysis combined with clustering algorithms to identify distinct customer groups. Five clustering methods were evaluated using Silhouette Score as the primary metric.

\begin{table}[H]
\centering
\caption{Clustering Model Comparison}
\label{tab:clustering_results}
\begin{tabular}{lrrr}
\toprule
\textbf{Model} & \textbf{Silhouette} & \textbf{Davies-Bouldin} & \textbf{Calinski-Harabasz} \\
\midrule
\textbf{DBSCAN} & \textbf{0.506} & \textbf{0.503} & 18.27 \\
K-Means & 0.288 & 1.170 & 295.81 \\
K-Means + PCA & 0.286 & 1.177 & 296.16 \\
GMM & 0.246 & 2.887 & 97.26 \\
Hierarchical & 0.231 & 1.299 & 246.66 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Best Model:} DBSCAN achieved the highest Silhouette Score (0.506) and lowest Davies-Bouldin Index (0.503), indicating well-separated and compact clusters. The algorithm identified 2 primary customer segments from the 793 unique customers.

\subsubsection{Performance Against Targets}

The clustering Silhouette score exceeds both minimum and optimal targets:

\begin{itemize}
    \item \textbf{Silhouette:} 0.506 achieved vs. 0.30 minimum / 0.50 optimal ($\checkmark$)
\end{itemize}

\subsubsection{Cluster Profiles}

DBSCAN identified two distinct customer segments:

\begin{table}[H]
\centering
\caption{Customer Segment Profiles}
\label{tab:cluster_profiles}
\begin{tabular}{llrrrr}
\toprule
\textbf{Cluster} & \textbf{Label} & \textbf{Size} & \textbf{Avg Recency} & \textbf{Avg Frequency} & \textbf{Avg Monetary} \\
\midrule
C0 & Loyal Customers & 784 & Low & High & High \\
C1 & Lost/Inactive & 3 & High & Low & High \\
\bottomrule
\end{tabular}
\end{table}

\begin{itemize}
    \item \textbf{Cluster 0 (Loyal Customers, n=784):} The majority of customers fall into this segment, characterized by recent purchases, regular buying frequency, and varied monetary values. These customers represent the core active customer base.

    \item \textbf{Cluster 1 (Lost/Inactive, n=3):} A small group of high-value customers who have not purchased recently. Despite high historical monetary values, their high recency indicates they may have churned.
\end{itemize}

\subsubsection{Cluster Visualization}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{cluster_profiles.png}
\caption{Customer Segmentation Analysis: Cluster Profiles and Distributions}
\label{fig:cluster_profiles}
\end{figure}

\subsubsection{RFM Distribution Analysis}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{rfm_distribution.png}
\caption{Distribution of RFM Variables and Derived Features}
\label{fig:rfm_distribution}
\end{figure}

Key observations from the RFM distributions:
\begin{itemize}
    \item \textbf{Recency:} Right-skewed (skew=2.27), with most customers having purchased recently ($\mu=147.8$ days, median=76 days)
    \item \textbf{Frequency:} Approximately normal (skew=0.36), average of 6.3 orders per customer
    \item \textbf{Monetary:} Right-skewed (skew=2.47), with mean \$2,896.80 and median \$2,256.40
\end{itemize}

% =====================================================
% SECTION 5: DISCUSSION
% =====================================================
\section{Discussion}
\label{sec:discussion}

This section synthesizes the experimental results, discusses key findings across all four tasks, and presents business implications.

\subsection{Summary of Results}

\begin{table}[H]
\centering
\caption{Performance Summary vs. Targets}
\label{tab:summary}
\begin{tabular}{llrrr}
\toprule
\textbf{Task} & \textbf{Best Model} & \textbf{Primary Metric} & \textbf{Target} & \textbf{Status} \\
\midrule
Profit Regression & LightGBM & RMSE: \$208.80 & $<$\$200 & Near target \\
 & & $R^2$: 0.180 & $>$0.30 & Below target \\
Profitability Classification & Random Forest & F1: 0.965 & $>$0.75 & \textbf{Exceeded} \\
 & & ROC-AUC: 0.980 & $>$0.80 & \textbf{Exceeded} \\
Sales Forecasting & Prophet & MAPE: 19.21\% & $<$20\% & \textbf{Met} \\
Customer Segmentation & DBSCAN & Silhouette: 0.506 & $>$0.30 & \textbf{Exceeded} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Key Findings}

\subsubsection{Profit Prediction Insights}

The profit prediction task proved to be the most challenging, with the best model (LightGBM) achieving an RMSE of \$208.80 and $R^2$ of only 0.180. Key insights:

\begin{itemize}
    \item \textbf{Quantity is the strongest predictor:} Higher purchase quantities correlate with profit, though the relationship is complex.

    \item \textbf{Discount has a strong negative impact:} Confirming the EDA finding, discount erodes profitability significantly. The feature importance analysis shows Discount as one of the top 4 predictors.

    \item \textbf{Geographic and temporal patterns exist:} Postal Code, Order Day, and Order Month all rank highly, suggesting location-specific and seasonal profit patterns.

    \item \textbf{Product category matters:} Sub-categories like ``Machines'' have distinct profit profiles, indicating product-specific pricing strategies may be needed.

    \item \textbf{High variance limits predictability:} The presence of 18.8\% outliers in the Profit distribution (including extreme losses up to \$6,599) fundamentally limits model performance.
\end{itemize}

\subsubsection{Profitability Classification Insights}

The classification task achieved exceptional performance (F1 = 0.965, ROC-AUC = 0.980), far exceeding targets. Key insights:

\begin{itemize}
    \item \textbf{Transaction profitability is highly predictable:} The binary classification framing of the problem yields excellent results, making it suitable for real-world deployment.

    \item \textbf{Ensemble methods outperform linear models:} Random Forest achieved the best F1-score, while all tree-based methods significantly outperformed Logistic Regression.

    \item \textbf{High recall is achievable:} The 97.7\% recall of Random Forest means the model catches almost all unprofitable transactions, which is critical for business risk mitigation.

    \item \textbf{Practical deployment potential:} With such high accuracy, this model could be deployed to flag potentially unprofitable orders before processing, enabling intervention (e.g., discount adjustment, upselling).
\end{itemize}

\subsubsection{Sales Forecasting Insights}

Prophet achieved a MAPE of 19.21\%, meeting the target threshold. Key insights:

\begin{itemize}
    \item \textbf{Prophet handles seasonality well:} The model successfully captures Q4 sales spikes and monthly patterns, which traditional SARIMA and XGBoost methods struggled with.

    \item \textbf{Small sample limits model complexity:} With only 48 monthly observations, simpler models often outperform complex ones.

    \item \textbf{Traditional methods underperform:} SARIMA and XGBoost performed worse than the Seasonal Naive baseline in some respects, indicating overfitting to the limited training data.

    \item \textbf{Volatility is inherent:} Monthly sales show high variability, making sub-20\% MAPE a reasonable outcome.
\end{itemize}

\subsubsection{Customer Segmentation Insights}

DBSCAN clustering achieved a Silhouette Score of 0.506, exceeding the optimal target. Key insights:

\begin{itemize}
    \item \textbf{Natural customer separation exists:} The high Silhouette score indicates clear separation between customer groups based on RFM features.

    \item \textbf{Majority are loyal customers:} 99.6\% of customers (784 of 793) fall into the ``Loyal Customers'' segment, characterized by recent and frequent purchases.

    \item \textbf{Small at-risk segment identified:} 3 high-value customers show signs of churn (high recency despite historical high spending), presenting re-engagement opportunities.

    \item \textbf{DBSCAN outperforms parametric methods:} The density-based approach better handles the skewed RFM distributions compared to K-Means or GMM.
\end{itemize}

\subsection{Limitations}

\begin{enumerate}
    \item \textbf{Geographic scope:} Analysis is limited to United States data, limiting generalizability to other markets.

    \item \textbf{Temporal scope:} Four years of data may not capture long-term trends, economic cycles, or rare events.

    \item \textbf{Feature availability:} Key predictive features (marketing campaigns, competitor pricing, economic indicators, customer demographics) are not available.

    \item \textbf{Sample size for forecasting:} Only 48 monthly data points limit time series model complexity and validation rigor.

    \item \textbf{Profit variance:} The 18.8\% outlier rate in Profit makes regression inherently difficult; winsorizing was not applied in this analysis.

    \item \textbf{Customer granularity:} Segmentation identified only 2 distinct groups; finer segmentation may require additional features.
\end{enumerate}

\subsection{Business Implications}

Based on the experimental results, we recommend the following business actions:

\begin{enumerate}
    \item \textbf{Deploy profitability classifier:} The high-performing classification model (F1=0.965) should be integrated into the order processing system to flag potentially unprofitable transactions in real-time.

    \item \textbf{Review discount policies:} Discount has a strong negative correlation with profit and appears as a top feature in both regression and classification models. A structured discount approval process is recommended.

    \item \textbf{Use Prophet for sales planning:} The 19.21\% MAPE provides reasonable accuracy for inventory planning and resource allocation, particularly around Q4 peak periods.

    \item \textbf{Implement churn prevention:} The 3 identified at-risk customers represent high-value targets for re-engagement campaigns.

    \item \textbf{Investigate product-specific margins:} The feature importance analysis shows certain sub-categories (Machines, Binders, Tables, Copiers) have distinct profit profiles, warranting product-specific pricing reviews.

    \item \textbf{Consider geographic optimization:} Postal Code ranking highly suggests regional profit variations that could inform logistics and pricing decisions.
\end{enumerate}

% =====================================================
% SECTION 6: CONCLUSIONS
% =====================================================
\section{Conclusions and Future Work}
\label{sec:conclusions}

\subsection{Summary}

This study applied four distinct machine learning paradigms to a retail sales dataset of 9,994 transactions spanning 2014--2017. Key accomplishments:

\begin{enumerate}
    \item \textbf{Profit Regression:} LightGBM achieved the best performance (RMSE=\$208.80, $R^2$=0.180), with Quantity, Postal Code, and Discount as top predictors. While not meeting the $R^2$ target, the model provides directional insights into profit drivers.

    \item \textbf{Profitability Classification:} Random Forest achieved exceptional results (F1=0.965, ROC-AUC=0.980), significantly exceeding all targets. The model is deployment-ready for real-time transaction flagging.

    \item \textbf{Sales Forecasting:} Prophet achieved a MAPE of 19.21\%, meeting the target threshold and outperforming SARIMA and XGBoost baselines. The model captures seasonal patterns effectively.

    \item \textbf{Customer Segmentation:} DBSCAN achieved a Silhouette Score of 0.506, exceeding the optimal target and identifying two distinct customer groups (Loyal Customers and Lost/Inactive).
\end{enumerate}

\subsection{Recommendations}

Based on the experimental results, we recommend the following prioritized actions:

\begin{enumerate}
    \item \textbf{High Priority:} Deploy the profitability classifier to flag at-risk transactions before processing.

    \item \textbf{High Priority:} Implement a structured discount approval workflow, limiting discounts $>20\%$ to manager approval.

    \item \textbf{Medium Priority:} Integrate Prophet forecasts into Q4 inventory planning and staffing decisions.

    \item \textbf{Medium Priority:} Launch targeted re-engagement campaigns for the 3 identified at-risk customers.

    \item \textbf{Lower Priority:} Conduct product-specific margin analysis for sub-categories with high feature importance (Machines, Binders, Tables, Copiers, Appliances).
\end{enumerate}

\subsection{Future Work}

\begin{enumerate}
    \item \textbf{Feature enrichment:} Integration of external data sources (economic indicators, competitor pricing, marketing campaign data) to improve profit prediction $R^2$.

    \item \textbf{Real-time deployment:} Productionize the classification model as a microservice with REST API for integration into order management systems.

    \item \textbf{Advanced forecasting:} Explore neural network approaches (LSTM, Transformer) with additional data collection to improve MAPE below 15\%.

    \item \textbf{A/B testing:} Design and execute segment-specific marketing experiments based on customer clusters.

    \item \textbf{Outlier treatment:} Investigate winsorizing or robust regression methods to improve profit prediction accuracy.

    \item \textbf{Finer segmentation:} Collect additional customer data (demographics, engagement metrics) to enable more granular segmentation.
\end{enumerate}

% =====================================================
% APPENDIX
% =====================================================
\appendix

\section{Hyperparameter Configurations}
\label{app:hyperparams}

\subsection{ProfitRegressor (LightGBM)}

\begin{table}[H]
\centering
\caption{LightGBM Regressor Hyperparameters}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
\texttt{num\_leaves} & 50 \\
\texttt{n\_estimators} & 300 \\
\texttt{learning\_rate} & 0.01 \\
\texttt{feature\_fraction} & 1.0 \\
\texttt{random\_state} & 42 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{ProfitabilityClassifier (Random Forest)}

\begin{table}[H]
\centering
\caption{Random Forest Classifier Hyperparameters}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
\texttt{n\_estimators} & 100 (default) \\
\texttt{max\_depth} & None \\
\texttt{class\_weight} & balanced \\
\texttt{random\_state} & 42 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{SalesForecaster (Prophet)}

\begin{table}[H]
\centering
\caption{Prophet Hyperparameters}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
\texttt{changepoint\_prior\_scale} & 0.1 \\
\texttt{seasonality\_mode} & additive \\
\texttt{yearly\_seasonality} & auto \\
\texttt{weekly\_seasonality} & False \\
\bottomrule
\end{tabular}
\end{table}

\subsection{CustomerSegmenter (DBSCAN)}

\begin{table}[H]
\centering
\caption{DBSCAN Hyperparameters}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
\texttt{eps} & auto-tuned \\
\texttt{min\_samples} & 5 \\
\texttt{metric} & euclidean \\
\bottomrule
\end{tabular}
\end{table}

\section{Technical Environment}
\label{app:environment}

\begin{table}[H]
\centering
\caption{Software Environment}
\begin{tabular}{ll}
\toprule
\textbf{Component} & \textbf{Version} \\
\midrule
Python & 3.x \\
pandas & Latest \\
scikit-learn & Latest \\
XGBoost & Latest \\
LightGBM & Latest \\
Prophet & Latest \\
\bottomrule
\end{tabular}
\end{table}

\begin{itemize}
    \item \textbf{Random State:} 42 (all experiments for reproducibility)
    \item \textbf{Hardware:} Apple Silicon (M-series) with MPS acceleration where applicable
    \item \textbf{Parallelization:} Dynamic \texttt{n\_jobs} allocation based on concurrent agent count
    \item \textbf{Cross-Validation:} 5-fold (stratified for classification, temporal split for forecasting)
\end{itemize}

\section{Generated Outputs}
\label{app:outputs}

The following artifacts were generated during this analysis:

\begin{table}[H]
\centering
\caption{Generated Model and Data Files}
\begin{tabular}{ll}
\toprule
\textbf{File} & \textbf{Description} \\
\midrule
\texttt{profit\_regressor\_best\_model.pkl} & Trained LightGBM regressor \\
\texttt{profit\_regressor\_results.csv} & Model comparison metrics \\
\texttt{profit\_regressor\_feature\_importance.png} & Feature importance plot \\
\texttt{sales\_forecaster\_best\_model.pkl} & Trained Prophet model \\
\texttt{sales\_forecast\_predictions.csv} & Test set predictions \\
\texttt{sales\_forecast\_plot.png} & Forecast visualization \\
\texttt{customer\_segmenter\_model.pkl} & Trained DBSCAN model \\
\texttt{customer\_segments.csv} & Customer-to-cluster mapping \\
\texttt{cluster\_profiles.png} & Cluster visualization \\
\texttt{rfm\_distribution.png} & RFM feature distributions \\
\bottomrule
\end{tabular}
\end{table}

% =====================================================
% BIBLIOGRAPHY
% =====================================================
\begin{thebibliography}{9}

\bibitem{xgboost}
Chen, T., \& Guestrin, C. (2016). XGBoost: A scalable tree boosting system. In \emph{Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining} (pp. 785--794).

\bibitem{lightgbm}
Ke, G., et al. (2017). LightGBM: A highly efficient gradient boosting decision tree. \emph{Advances in Neural Information Processing Systems}, 30.

\bibitem{prophet}
Taylor, S. J., \& Letham, B. (2018). Forecasting at scale. \emph{The American Statistician}, 72(1), 37--45.

\bibitem{rfm}
Hughes, A. M. (1994). \emph{Strategic database marketing}. Probus Publishing Company.

\bibitem{kmeans}
Arthur, D., \& Vassilvitskii, S. (2007). k-means++: The advantages of careful seeding. In \emph{Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms} (pp. 1027--1035).

\end{thebibliography}

\end{document}
